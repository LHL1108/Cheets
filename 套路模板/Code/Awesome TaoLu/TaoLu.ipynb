{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库文件\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "sns.set(style='white', context='notebook', palette='Set2')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import gc \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生采样数据\n",
    "def genarate_sample_data(begin):\n",
    "    with open('train_df.pkl', 'rb') as f: \n",
    "        train_df = pickle.load(f)\n",
    "        \n",
    "    with open('test_df.pkl', 'rb') as f:\n",
    "        test_df = pickle.load(f)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生测试数据\n",
    "def create_test_df(begin):\n",
    "    if begin == True:\n",
    "        test_data = pd.read_csv('../input/test_V2.csv')\n",
    "        test_df = reduce_mem_usage(test_data)\n",
    "        \n",
    "        del test_data\n",
    "        gc.collect()\n",
    "        \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入莺尾花数据集\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris() # 导入数据集\n",
    "X = iris.data # 获得其特征向量\n",
    "y = iris.target # 获得样本label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自制数据集\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=6, n_features=5, n_informative=2, \n",
    "    n_redundant=2, n_classes=2, n_clusters_per_class=2, scale=1.0, \n",
    "    random_state=20)\n",
    "\n",
    "# n_samples：指定样本数\n",
    "# n_features：指定特征数\n",
    "# n_classes：指定几分类\n",
    "# random_state：随机种子，使得随机状可重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas备忘手册"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 基本属性\n",
    "df.dtypes：            data type of columns列数据类型\n",
    "df.index ：             indexes行索引\n",
    "df.columns：         return pandas.Index列名称（label）\n",
    "df.values：            each row, return array[array]\n",
    "df.shape：            a tuple representing the dimensionality of df\n",
    "\n",
    "# 导入数据\n",
    "pd.read_csv(filename)：从CSV文件导入数据\n",
    "pd.read_table(filename)：从限定分隔符的文本文件导入数据\n",
    "pd.read_excel(filename)：从Excel文件导入数据\n",
    "pd.read_sql(query, connection_object)：从SQL表/库导入数据\n",
    "pd.read_json(json_string)：从JSON格式的字符串导入数据\n",
    "pd.read_html(url)：解析URL、字符串或者HTML文件，抽取其中的tables表格\n",
    "pd.read_clipboard()：从你的粘贴板获取内容，并传给read_table()\n",
    "pd.DataFrame(dict)：从字典对象导入数据，Key是列名，Value是数据\n",
    "\n",
    "# 导出数据\n",
    "df.to_csv(filename)：导出数据到CSV文件\n",
    "df.to_excel(filename)：导出数据到Excel文件\n",
    "df.to_sql(table_name, connection_object)：导出数据到SQL表\n",
    "df.to_json(filename)：以Json格式导出数据到文本文件\n",
    "\n",
    "# 创建测试对象\n",
    "pd.DataFrame(np.random.rand(20,5))：创建20行5列的随机数组成的DataFrame对象\n",
    "pd.Series(my_list)：从可迭代对象my_list创建一个Series对象\n",
    "df.index = pd.date_range('1900/1/30', periods=df.shape[0])：增加一个日期索引\n",
    "\n",
    "# 查看检查数据\n",
    "df.head(n)：查看DataFrame对象的前n行\n",
    "df.tail(n)：查看DataFrame对象的最后n行\n",
    "df.shape()：查看行数和列数\n",
    "df.info() ：查看索引、数据类型和内存信息\n",
    "df.describe()：查看数值型列的汇总统计\n",
    "s.value_counts(dropna=False)：查看Series对象的唯一值和计数\n",
    "df.apply(pd.Series.value_counts)：查看DataFrame对象中每一列的唯一值和计数\n",
    "\n",
    "# 选取数据\n",
    "df[col]：根据列名，并以Series的形式返回列\n",
    "df[[col1, col2]]：以DataFrame形式返回多列\n",
    "s.iloc[0]：按位置(行号)选取行数据此处为第1行，第一列则为s.iloc[:,0]\n",
    "df.loc['index_one']：按索引（行标签）选取行数据\n",
    "df.loc[0,'ID']：返回第0行，索引为'ID'的列的元素\n",
    "df.iloc[0,:]：返回第一行\n",
    "df.iloc[:,0]：返回第一列\n",
    "df.iloc[0,0]：返回第一列的第一个元素\n",
    "df.ix[0]：ix为loc和iloc的综合(很多新版本已经弃用)\n",
    "\n",
    "# 数据清洗\n",
    "col = [\"id\"，\"tile\"]\n",
    "df1 = pd.DataFrame(df,columns = col) :切分df\n",
    "df.columns = ['a','b','c']：重命名列名\n",
    "pd.isnull()：检查DataFrame对象中的空值，并返回一个Boolean数组\n",
    "pd.notnull()：检查DataFrame对象中的非空值，并返回一个Boolean数组\n",
    "df.dropna()：删除所有包含空值的行\n",
    "df.dropna(axis=1)：删除所有包含空值的列\n",
    "df.dropna(axis=1,thresh=n)：删除所有小于n个非空值的行\n",
    "df.fillna(x)：用x替换DataFrame对象中所有的空值\n",
    "s.astype(float)：将Series中的数据类型更改为float类型\n",
    "s.replace(1,'one')：用‘one’代替所有等于1的值\n",
    "s.replace([1,3],['one','three'])：用'one'代替1，用'three'代替3\n",
    "df.rename(columns=lambda x: x + 1)：批量更改列名\n",
    "df.rename(columns={'old_name': 'new_ name'})：选择性更改列名\n",
    "df.set_index('column_one')：更改索引列\n",
    "df.rename(index=lambda x: x + 1)：批量重命名索引\n",
    "\n",
    "# 数据处理\n",
    "df[df[col] > 0.5]：选择col列的值大于0.5的行\n",
    "df.sort_values(col1)：按照列col1排序数据，默认升序排列\n",
    "df.sort_values(col2, ascending=False)：按照列col1降序排列数据\n",
    "df.sort_values([col1,col2], ascending=[True,False])：先按列col1升序排列，后按col2降序排列数据\n",
    "df.groupby(col)：返回一个按列col进行分组的Groupby对象\n",
    "df.groupby([col1,col2])：返回一个按多列进行分组的Groupby对象\n",
    "df.groupby(col1)[col2]：返回按列col1进行分组后，列col2的均值\n",
    "df.pivot_table(index=col1, values=[col2,col3], aggfunc=max)：创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表\n",
    "df.groupby(col1).agg(np.mean)：返回按列col1分组的所有列的均值\n",
    "data.apply(np.mean)：对DataFrame中的每一列应用函数np.mean\n",
    "data.apply(np.max,axis=1)：对DataFrame中的每一行应用函数np.max\n",
    "\n",
    "# 数据合并\n",
    "df1.append(df2)：将df2中的行添加到df1的尾部\n",
    "df.concat([df1, df2],axis=1)：将df2中的列添加到df1的尾部\n",
    "df1.join(df2,on=col1,how='inner')：对df1的列和df2的列执行SQL形式的join\n",
    "\n",
    "# 数据统计\n",
    "df.describe()：查看数据值列的汇总统计\n",
    "df.mean()：返回所有列的均值\n",
    "df.corr()：返回列与列之间的相关系数\n",
    "df.count()：返回每一列中的非空值的个数\n",
    "df.max()：返回每一列的最大值\n",
    "df.min()：返回每一列的最小值\n",
    "df.median()：返回每一列的中位数\n",
    "df.std()：返回每一列的标准差\n",
    "\n",
    "# 数据变化\n",
    "1.apply()\n",
    "当想让方程作用在一维的向量上时，可以使用apply来完成\n",
    "但是因为大多数的列表统计方程 (比如 sum 和 mean)是DataFrame的函数，所以apply很多时候不是必须的\n",
    "2.applymap()\n",
    "如果想让方程作用于DataFrame中的每一个元素，可以使用applymap().\n",
    "3.map()\n",
    "map()只要是作用将函数作用于一个Series的每一个元素\n",
    "4.replace函數：\n",
    "语法：replace(self, to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad', axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据变换代码示例\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "frame = pd.DataFrame(np.random.randn(4, 3), columns=list('abc'),index=['a', 'b', 'c','d'])\n",
    "print (frame)\n",
    "#          a         b         c\n",
    "#a -0.074178  0.217576  0.489068\n",
    "#b  0.922744  1.524651  2.127485\n",
    "#c -1.457947 -0.498123 -0.687133\n",
    "#d  0.106064 -2.890129  0.981858\n",
    "\n",
    "f = lambda x: x.max() - x.min()\n",
    "print(frame.apply(f))\n",
    "\n",
    "\n",
    "format = lambda x: x * x\n",
    "print(frame.applymap(format))\n",
    "\n",
    "#          a         b         c\n",
    "#a  0.005502  0.047339  0.239188\n",
    "#b  0.851457  2.324562  4.526193\n",
    "#c  2.125609  0.248126  0.472152\n",
    "#d  0.011250  8.352848  0.964045\n",
    "\n",
    "print(frame['a'].map(format))\n",
    "#a    0.005502\n",
    "#b    0.851457\n",
    "#c    2.125609\n",
    "#d    0.011250\n",
    "#Name: a, dtype: float64\n",
    "\n",
    "作者：wong小尧\n",
    "链接：https://www.jianshu.com/p/f5d6423709fc\n",
    "來源：简书\n",
    "简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "# 1. 基于mean和std的标准化\n",
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "scaler.transform(train_data)\n",
    "scaler.transform(test_data)\n",
    "\n",
    "# 2. 将每个特征值归一化到一个固定范围\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(train_data)\n",
    "scaler.transform(train_data)\n",
    "scaler.transform(test_data)\n",
    "#feature_range: 定义归一化范围，注用（）括起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_normalized = preprocessing.normalize(X, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]\n",
    "encoder = preprocessing.OneHotEncoder().fit(data)\n",
    "enc.transform(data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mode_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合模型\n",
    "model.fit(X_train, y_train)\n",
    "# 模型预测\n",
    "model.predict(X_test)\n",
    "\n",
    "# 获得这个模型的参数\n",
    "model.get_params()\n",
    "# 为模型进行打分\n",
    "model.score(data_X, data_y) # 线性回归：R square； 分类问题： acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# 定义线性回归模型\n",
    "model = LinearRegression(fit_intercept=True, normalize=False, \n",
    "    copy_X=True, n_jobs=1)\n",
    "\"\"\"\n",
    "参数\n",
    "---\n",
    "    fit_intercept：是否计算截距。False-模型没有截距\n",
    "    normalize： 当fit_intercept设置为False时，该参数将被忽略。 如果为真，则回归前的回归系数X将通过减去平均值并除以l2-范数而归一化。\n",
    "     n_jobs：指定线程数\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# 定义逻辑回归模型\n",
    "model = LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, \n",
    "    fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "    random_state=None, solver=’liblinear’, max_iter=100, multi_class=’ovr’, \n",
    "    verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "\"\"\"参数\n",
    "---\n",
    "    penalty：使用指定正则化项（默认：l2）\n",
    "    dual: n_samples > n_features取False（默认）\n",
    "    C：正则化强度的反，值越小正则化强度越大\n",
    "    n_jobs: 指定线程数\n",
    "    random_state：随机数生成器\n",
    "    fit_intercept: 是否需要常量\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "model = naive_bayes.GaussianNB() # 高斯贝叶斯\n",
    "model = naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "model = naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "\"\"\"\n",
    "文本分类问题常用MultinomialNB\n",
    "参数\n",
    "---\n",
    "    alpha：平滑参数\n",
    "    fit_prior：是否要学习类的先验概率；false-使用统一的先验概率\n",
    "    class_prior: 是否指定类的先验概率；若指定则不能根据参数调整\n",
    "    binarize: 二值化的阈值，若为None，则假设输入由二进制向量组成\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "model = tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None, \n",
    "    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "    max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "     class_weight=None, presort=False)\n",
    "\"\"\"参数\n",
    "---\n",
    "    criterion ：特征选择准则gini/entropy\n",
    "    max_depth：树的最大深度，None-尽量下分\n",
    "    min_samples_split：分裂内部节点，所需要的最小样本树\n",
    "    min_samples_leaf：叶子节点所需要的最小样本数\n",
    "    max_features: 寻找最优分割点时的最大特征数\n",
    "    max_leaf_nodes：优先增长到最大叶子节点数\n",
    "    min_impurity_decrease：如果这种分离导致杂质的减少大于或等于这个值，则节点将被拆分。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(C=1.0, kernel=’rbf’, gamma=’auto’)\n",
    "\"\"\"参数\n",
    "---\n",
    "    C：误差项的惩罚参数C\n",
    "    gamma: 核相关系数。浮点数，If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "#定义kNN分类模型\n",
    "model = neighbors.KNeighborsClassifier(n_neighbors=5, n_jobs=1) # 分类\n",
    "model = neighbors.KNeighborsRegressor(n_neighbors=5, n_jobs=1) # 回归\n",
    "\"\"\"参数\n",
    "---\n",
    "    n_neighbors： 使用邻居的数目\n",
    "    n_jobs：并行任务数\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多层感知机MLP(DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# 定义多层感知机分类算法\n",
    "model = MLPClassifier(activation='relu', solver='adam', alpha=0.0001)\n",
    "\"\"\"参数\n",
    "---\n",
    "    hidden_layer_sizes: 元祖\n",
    "    activation：激活函数\n",
    "    solver ：优化算法{‘lbfgs’, ‘sgd’, ‘adam’}\n",
    "    alpha：L2惩罚(正则化项)参数。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  模型评估与选择篇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X, y=None, scoring=None, cv=None, n_jobs=1)\n",
    "\"\"\"参数\n",
    "---\n",
    "    model：拟合数据的模型\n",
    "    cv ： k-fold\n",
    "    scoring: 打分参数-‘accuracy’、‘f1’、‘precision’、‘recall’ 、‘roc_auc’、'neg_log_loss'等等\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检验曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用检验曲线，我们可以更加方便的改变模型参数，获取模型表现。\n",
    "from sklearn.model_selection import validation_curve\n",
    "train_score, test_score = validation_curve(model, X, y, param_name, param_range, cv=None, scoring=None, n_jobs=1)\n",
    "\"\"\"参数\n",
    "---\n",
    "    model:用于fit和predict的对象\n",
    "    X, y: 训练集的特征和标签\n",
    "    param_name：将被改变的参数的名字\n",
    "    param_range： 参数的改变范围\n",
    "    cv：k-fold\n",
    "   \n",
    "返回值\n",
    "---\n",
    "   train_score: 训练集得分（array）\n",
    "    test_score: 验证集得分（array）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 保存模型\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# 读取模型\n",
    "with open('model.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(model, 'model.pickle')\n",
    "\n",
    "#载入模型\n",
    "model = joblib.load('model.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 缺失值检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'train':train_df.isnull().sum(),'test':test_df.isnull().sum()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列化保存模型\n",
    "with open('model001.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# 逆序列化读取模型\n",
    "with open('model001.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_by_team(df):\n",
    "    df_mean = df.groupby(by=['matchId', 'groupId'])[feature_cols].mean()\n",
    "    df_mean_rank = df_mean.groupby(by=['matchId', 'groupId']).rank(pct=True).reset_index()\n",
    "    df = df.merge(df_mean_rank, on=['matchId', 'groupId'], how='left', suffixes=['', '_mean_rank'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义划分时间段函数\n",
    "def getSeg(x):\n",
    "    if x >=0 and x <=6:\n",
    "        return 1\n",
    "    elif x >=7 and x <= 12:\n",
    "        return 2 \n",
    "    elif x >=13 and x <=18:\n",
    "        return 3 \n",
    "    elif x >= 19 and x <=23:\n",
    "        return 4 \n",
    "\n",
    "train_df['hour_seg'] = train_df['hour'].apply(lambda x: getSeg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对两个类别型特征求交叉特征后的count\n",
    "# add cross feature \n",
    "first_feature = ['app_cate_id', 'f_channel', 'app_id']\n",
    "second_feature = ['make', 'model', 'osv1', 'osv2', 'osv3', 'adid', 'advert_name', 'creative_id'\n",
    "                 'carrier', 'nnt', 'devtype', 'os']\n",
    "cross_feature = [] \n",
    "for feat_1 in first_feature:\n",
    "    for feat_2 in second_feature:\n",
    "        col_name = 'cross_' + feat_1 + '_and_' + 'feat_2'\n",
    "        cross_feature.append(col_name)\n",
    "        data[col_name] = data[feat_1].astype(str).values + '_' + data[feat_2].astype(str).values\n",
    "\n",
    "# 求count计数特征\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建类别特征的nunique特征(如广告主id有多少个不同的广告id)\n",
    "adid_nuq = ['moedel', 'make', 'os', 'city', 'province', 'user_tags', 'f_channel', 'app_id', 'carrier', 'nnt', 'dtype', 'app_cate_id', 'inner_slot_id']\n",
    "for feat in adid_nuq:\n",
    "    gp1 = data.groupby('adid')[feat].unique().reset_index().rename(columns={feat:'adid_%snuq_num' % feat})\n",
    "    gp2 = data.groupby(feat)['adid'].unique().reset_index().rename(columns={'adid': '%s_adid_nuq_num' % feat})\n",
    "    data = pd.merge(data, gp1, how='left', on=['adid'])\n",
    "    data = pd.merge(data, gp2, hpw='left', on=[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 减小内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 减小内存的具体方法,参考https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" 遍历dataFrame的每一列并修改其中的数据类型可以减小内存       \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建所有特征按group分组再求平均后的百分比排名作为新的特征\n",
    "def rank_by_team(df):\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    features = [col for col in df.columns if col not in cols_to_drop]\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n",
    "    agg = agg.groupby('matchId').rank(pct=True)\n",
    "    return df.merge(agg, suffixes=['', '_mean_rank'], how='left', on=['matchId', 'groupId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产出特征和特征名称\n",
    "def feature_creation(df):\n",
    "   # 定义保留列\n",
    "    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\n",
    "    feature_cols = [x for x in df.columns if x not in cols_to_drop]\n",
    "    \n",
    "    # 创造排序特征并去除一些无用列\n",
    "    rank_df = rank_by_team(df)\n",
    "    cols = [x for x in rank_df.columns if x not in cols_to_drop]\n",
    "    rank_df = rank_df[cols]\n",
    "    features_name = rank_df.columns\n",
    "    X = rank_df.values\n",
    "    \n",
    "    return X, features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#产出标签\n",
    "def label_creation(df):\n",
    "    y = df[['winPlacePerc']]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制特征重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制特种重要度\n",
    "def plot_feature_importance(model):\n",
    "    importance = model.feature_importances_\n",
    "    feature = features_name\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title('Feature Importance')\n",
    "    feature_importance = pd.DataFrame(importance)\n",
    "    feature_importance.columns = ['importance']\n",
    "    feature_importance['feature'] = feature \n",
    "    feature_importance = feature_importance[['feature', 'importance']]\n",
    "    feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "    sns.barplot(data=feature_importance, y='feature', x='importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_{}.png'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于卡方检验的特征重要度选择\n",
    "# 卡方检验\n",
    "SKB = SelectPercentile(chi2, percentile=95).fit(train_new, train_y)\n",
    "train_new = SKB.transform(train_new)\n",
    "test_new = SKB.transform(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生训练数据\n",
    "def create_train_df(begin):\n",
    "    if begin == True:\n",
    "        train_data = pd.read_csv('../input/train_V2.csv')\n",
    "        train_df = reduce_mem_usage(train_data)\n",
    "        train_df = train_df.sort_values(by=['matchId', 'groupId']).iloc[:1000000, :]\n",
    "        \n",
    "        del train_data\n",
    "        gc.collect()\n",
    "        \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取全部数据\n",
    "def read_data(begin):\n",
    "    if begin == True:\n",
    "        train_df = pd.read_csv('../input/train_V2.csv')\n",
    "        test_df = pd.read_csv('../input/test_V2.csv')\n",
    "    else:\n",
    "        print('未读取数据')\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制相关性热力图\n",
    "drop_features = ['Id', 'groupId', 'matchId']\n",
    "feats = [f for f in train_df.columns if f not in drop_features]\n",
    "plt.figure(figsize=(18, 16))\n",
    "sns.heatmap(train_df[feats].corr(),vmax=1.0, annot=True, square=True, linewidths=0.1, linecolor='black', cmap='RdBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "params = {\n",
    "    'num_leaves': 144,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 800,\n",
    "    'max_depth':12,\n",
    "    'max_bin':55,\n",
    "    'bagging_fraction':0.8,\n",
    "    'bagging_freq':5,\n",
    "    'feature_fraction':0.9,\n",
    "    'verbose':50, \n",
    "    'early_stopping_rounds':100\n",
    "    }\n",
    "\n",
    "# LightGBM parameters\n",
    "model = lgb.LGBMRegressor(num_leaves=params['num_leaves'], learning_rate=params['learning_rate'], \n",
    "                    n_estimators=params['n_estimators'], max_depth=params['max_depth'],\n",
    "                    max_bin = params['max_bin'], bagging_fraction = params['bagging_fraction'], \n",
    "                    bagging_freq = params['bagging_freq'], feature_fraction = params['feature_fraction'],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型、清理内存\n",
    "with open('new_model_{}.pkl'.format(model_name), 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "del X_train, X_valid, y_train, y_valid \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数及五折构造结果\n",
    "lgb_clf = lgb..LGBMClassifier(boosting_type='gbdt', num_leaves=48, max_depth=-1, learning_rate=0.02, n_estimators=6000, max_bin=425, subsample_for_bin=50000, objective='binary', min_split_gain=0, min_child_weight=5, min_child_samples=10, subsample=0.8, subsample_freq=1, colsample_bytree=0.8, reg_alpha=3, reg_lambda=0.1, seed=1000, n_jobs=-1, silent=True)\n",
    "skf = list(StratifiedKFold(y_train, n_folds=5, shuffle=True, random_state=1024))\n",
    "baseloss = [] \n",
    "loss = 0 \n",
    "for i, (train_index, test_index) in enumerate(skf):\n",
    "    print('Fold', i)\n",
    "    lgb_model = lgb_clf.fit(X_train[train_index], y_train[train_index],\n",
    "                           eval_names=['train', 'valid'],\n",
    "                           eval_metric='logloss',\n",
    "                           eval_set=[(X_train[train_index], y_train[train_index]), (X_train[test_index], y_train[test_index]), early_stopping_rounds=100])\n",
    "    baseloss.append(lgb_model.best_score_['valid']['binary_logloss'])\n",
    "    loss += lgb_model.best_score_['valid']['binary_logloss']\n",
    "    test_pred = lgb_model.predict_proba(X_test, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    print('test mean:', test_pred.mean())\n",
    "    res['prob_%s' % str(i)] = test_pred \n",
    "print('logloss:', baseloss, loss/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测并产出结果\n",
    "\n",
    "# 开始读取训练数据\n",
    "test_df = create_test_df(begin)\n",
    "#train_df, test_df = genarate_sample_data(begin)\n",
    "# 数据变换和预测\n",
    "test_df = drop_na(test_df)\n",
    "X_test, features_name = feature_creation(test_df)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 产出结果\n",
    "submission = pd.concat([test_df['Id'], pd.DataFrame(y_pred, columns=['winPlacePerc'])], axis=1)\n",
    "submission['winPlacePerc'][submission['winPlacePerc']>1] == 1\n",
    "submission.to_csv('../output/submission_{}.csv'.format(model_name), index=False)\n",
    "print('产出结果成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分层抽样\n",
    "from sklearn.cross_validation import KFold \n",
    "eval_size = 0.10 \n",
    "kf = KFold(len(y), round(1. / eval_size))\n",
    "train_indices, valid_indices = next(iter(kf))\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_valid, y_valid = X[valid_indices], y[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建回归类问题数据\n",
    "import numpy as np \n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#create some data \n",
    "X = np.linspace(-1, 1, 200)\n",
    "np.random.shuffle(X)\n",
    "# 最后训练出的结果，w越接近1, b越接近2 ,效果越好\n",
    "y = 1 * X + 2 + np.random.normal(0, 0.05, (200,))\n",
    "\n",
    "#plot data \n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras搭建DNN解决回归类问题模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#part1: train data  \n",
    "#generate 100 numbers from -2pi to 2pi\n",
    "x_train = np.linspace(-2*np.pi, 2*np.pi, 1000)#array: [1000,]  \n",
    "x_train = np.array(x_train).reshape((len(x_train), 1)) #reshape to matrix with [100,1]\n",
    "n=0.1*np.random.rand(len(x_train),1) #generate a matrix with size [len(x),1], value in (0,1),array: [1000,1]  \n",
    "y_train=np.sin(x_train)+n\n",
    "\n",
    "#训练数据集：零均值单位方差\n",
    "x_train = preprocessing.scale(x_train)\n",
    "scaler = preprocessing.StandardScaler().fit(x_train) \n",
    "y_train = scaler.transform(y_train)\n",
    "\n",
    "#part2: test data  \n",
    "x_test = np.linspace(-5,5,2000)\n",
    "x_test = np.array(x_test).reshape((len(x_test), 1))\n",
    "y_test=np.sin(x_test)\n",
    " \n",
    "#零均值单位方差\n",
    "x_test = scaler.transform(x_test)\n",
    "y_test = scaler.transform(y_test)\n",
    "#plot testing data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, y_test,'g')\n",
    "\n",
    "#prediction data\n",
    "x_prd = np.linspace(-3,3,101)\n",
    "x_prd = np.array(x_prd).reshape((len(x_prd), 1))\n",
    "x_prd = scaler.transform(x_prd)\n",
    "y_prd=np.sin(x_prd)\n",
    "#plot testing data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_prd, y_prd,'r')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, init='uniform', input_dim=1))\n",
    "#model.add(Activation(LeakyReLU(alpha=0.01))) \n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(50))\n",
    "#model.add(Activation(LeakyReLU(alpha=0.1))) \n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "#model.add(Activation(LeakyReLU(alpha=0.01))) \n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
    "#model.compile(loss='mean_squared_error', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "#model.fit(x_train, y_train, nb_epoch=64, batch_size=20, verbose=0) \n",
    "hist = model.fit(x_test, y_test, batch_size=10, nb_epoch=100, shuffle=True, verbose=0, validation_split=0.2)\n",
    "#print(hist.history)\n",
    "score = model.evaluate(x_test, y_test, batch_size=10)\n",
    "\n",
    "# 进行预测\n",
    "y_pred = model.predict(x_prd, batch_size=1)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN baseline2\n",
    "# 导入库文件\n",
    "import pandas as pd \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "import warnings \n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "begin = True \n",
    "\n",
    "# 产生采样数据\n",
    "def genarate_sample_data(begin):\n",
    "    with open('train_df.pkl', 'rb') as f: \n",
    "        train_df = pickle.load(f)\n",
    "        \n",
    "    with open('test_df.pkl', 'rb') as f:\n",
    "        test_df = pickle.load(f)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = genarate_sample_data(begin)\n",
    "\n",
    "train_df = train_df.drop(columns=['Id', 'groupId', 'matchId', 'matchType'])\n",
    "test_df = test_df.drop(columns=['Id', 'groupId', 'matchId', 'matchType'])\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "X = train_df.drop(columns=['winPlacePerc']).iloc[:1000, :]\n",
    "y = train_df[['winPlacePerc']].iloc[:1000,:]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "X_test = test_df.iloc[:1000,:]\n",
    "\n",
    "X_test.shape\n",
    "\n",
    "# 搭建模型\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='tanh', input_dim=24))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(loss='mean_absolute_error', optimizer='rmsprop')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=8, epochs=10)\n",
    "\n",
    "# 训练集误差\n",
    "model.evaluate(X_train, y_train, batch_size=8)\n",
    "\n",
    "# 验证集误差\n",
    "model.evaluate(X_valid, y_valid, batch_size=8)\n",
    "\n",
    "# 对比验证数据和预测数据\n",
    "y_pred = model.predict(x=X_valid, batch_size=3)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred = y_pred.reset_index().iloc[:,1:]\n",
    "y_valid = y_valid.reset_index().iloc[:,1:]\n",
    "df = pd.concat([y_valid, y_pred], axis=1)\n",
    "df.columns = ['winPlacePerc', 'Perdiction']\n",
    "df.head()\n",
    "\n",
    "# 预测测试数据\n",
    "y_pred = model.predict(X_test)\n",
    "df = pd.DataFrame(y_pred)\n",
    "df.columns = ['prediction']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.leiphone.com/news/201709/zYIOJqMzR0mJARzj.html\n",
    "\n",
    "# 手动实现stacking操作\n",
    "# Out-of-Fold Predictions \n",
    "ntrain = train.shape[0]  # 891\n",
    "ntest = test.shape[0]    # 428\n",
    "kf = KFold(n_splits=5, random_state=2017)\n",
    "\n",
    "def get_ood(clf, X_train, y_train, X_test):\n",
    "    oof_train = np.zeros((ntrain,)) # 1 * 891\n",
    "    oof_test = np.zeros((ntest,)) # 1 * 418\n",
    "    oof_test_skf = np.empty((5, ntest)) # 5 * 418\n",
    "    \n",
    "    for i,(train_index, test_index) in enumerate(kf.split(X_train)): # X_train:891 * 7\n",
    "        kf_X_train = X_train[train_index]\n",
    "        kf_y_train = y_train[train_index]\n",
    "        kf_X_test = X_train[test_index]\n",
    "        \n",
    "        clf.train(kf_X_train, kf_y_train)\n",
    "        \n",
    "        oof_train[test_index] = clf.predict(kf_X_test)\n",
    "        oof_test_skf[i, :] = clf.predict(X_test)\n",
    "        \n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "    # oof_train.reshape(-1, 1): 891 * 1\n",
    "    # oof_test.reshape(-1, 1): 418 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "import numpy as np\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''5折stacking'''\n",
    "n_folds = 5\n",
    "skf = list(StratifiedKFold(y, n_folds))\n",
    "for j, clf in enumerate(clfs):\n",
    "    '''依次训练各个单模型'''\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
    "    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "'''融合使用的模型'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''切分训练数据集为d1,d2两部分'''\n",
    "X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2017)\n",
    "dataset_blend_train = np.zeros((X_d2.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    '''依次训练各个单模型'''\n",
    "    # print(j, clf)\n",
    "    '''使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。'''\n",
    "    # X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "    dataset_blend_train[:, j] = y_submission\n",
    "    '''对于测试集，直接用这k个模型的预测值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = clf.predict_proba(X_predict)[:, 1]\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n",
    "\n",
    "'''融合使用的模型'''\n",
    "# clf = LogisticRegression()\n",
    "clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(dataset_blend_train, y_test)\n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLXTEND模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【译】MLXTEND之StackingRegressor\n",
    "# https://www.jianshu.com/p/cc748e4f29c5?from=timeline\n",
    "\n",
    "# Stacking回归模型(不带交叉验证)\n",
    "\n",
    "# 使用波士顿数据集\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from mlxtend.data import boston_housing_data \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.svm import SVR \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "# 生成一个样本数据集\n",
    "np.random.seed(1)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - np.random.rand(8))\n",
    "\n",
    "# 初始化模型\n",
    "lr = LinearRegression() \n",
    "svr_lin = SVR(kernel='linear')\n",
    "ridge = Ridge(random_state=1)\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "\n",
    "# 融合四个模型\n",
    "model = StackingRegressor(regressors=[lr, svr_lin, ridge], meta_regressor=svr_rbf)\n",
    "\n",
    "# 训练stacking分类器\n",
    "model.fit(X, y)\n",
    "model.predict(X)\n",
    "\n",
    "# 拟合结果的评估和可视化\n",
    "print(\"Mean Squared Error: %.4f\" % np.mean((model.predict(X) - y) ** 2))\n",
    "print('Variance Score: %.4f' % model.score(X, y))\n",
    "with plt.style.context(('seaborn-whitegrid')):\n",
    "    plt.scatter(X, y, c='lightgray')\n",
    "    plt.plot(X, stregr.predict(X), c='darkgreen', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLXTEND Stacking回归网格搜索(不带交叉验证)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 初始化模型\n",
    "lr = LinearRegression()\n",
    "svr_lin = SVR(kernel='linear')\n",
    "ridge = Ridge(random_state=1)\n",
    "lasso = Lasso(random_state=1)\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "model = StackingRegressor(regressors=[svr_lin, lr, ridge, lasso], meta_regressor=svr_rbf)\n",
    "\n",
    "params = {\n",
    "    'lasso__alpha':[0.1, 1],\n",
    "    'ridge__alpha':[0.1, 1],\n",
    "    'svr__C':[0.1, 1],\n",
    "    'meta-svr__C':[0.1, 1],\n",
    "    'meta-svr__gamma':[0.1, 1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=params,\n",
    "                    cv=5, \n",
    "                    refit = True)\n",
    "\n",
    "grid.fit(X, y)\n",
    "for params, mean_score, scores in grid.grid_scores_:\n",
    "    print('%0.3f +/- %0.2f %r' % (mean_score, scores.std()/2.0, params))\n",
    "    \n",
    "# 拟合结果的评估和可视化\n",
    "print(\"Mean Squared Error: %.4f\"\n",
    "% np.mean((grid.predict(X) - y) ** 2))\n",
    "print('Variance Score: %.4f' % grid.score(X, y))\n",
    "with plt.style.context(('seaborn-whitegrid')):\n",
    "    plt.scatter(X, y, c='lightgray')\n",
    "    plt.plot(X, grid.predict(X), c='darkgreen', lw=2)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准交叉验证Stacking回归分类器\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "RANDOM_SEED = 42\n",
    "X, y = load_boston(return_X_y=True)\n",
    "svr = SVR(kernel='linear')\n",
    "lasso = Lasso()\n",
    "rf = RandomForestRegressor(n_estimators=5,\n",
    "random_state=RANDOM_SEED)\n",
    "# The StackingCVRegressor uses scikit-learn's check_cv\n",
    "# internally, which doesn't support a random seed. Thus\n",
    "# NumPy's random seed need to be specified explicitely for\n",
    "# deterministic behavior\n",
    "np.random.seed(RANDOM_SEED)\n",
    "stack = StackingCVRegressor(regressors=(svr, lasso, rf),\n",
    "meta_regressor=lasso)\n",
    "print('5-fold cross validation scores:\\n')\n",
    "for clf, label in zip([svr, lasso, rf, stack], ['SVM', 'Lasso','Random Forest','StackingClassifier']):\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(\"R^2 Score: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "# 5-fold cross validation scores:\n",
    "# R^2 Score: 0.45 (+/- 0.29) [SVM]\n",
    "# R^2 Score: 0.43 (+/- 0.14) [Lasso]\n",
    "# R^2 Score: 0.52 (+/- 0.28) [Random Forest]\n",
    "# R^2 Score: 0.58 (+/- 0.24) [StackingClassifier]\n",
    "# The StackingCVRegressor uses scikit-learn's check_cv\n",
    "# internally, which doesn't support a random seed. Thus\n",
    "# NumPy's random seed need to be specified explicitely for\n",
    "# deterministic behavior\n",
    "np.random.seed(RANDOM_SEED)\n",
    "stack = StackingCVRegressor(regressors=(svr, lasso, rf),\n",
    "meta_regressor=lasso)\n",
    "print('5-fold cross validation scores:\\n')\n",
    "for clf, label in zip([svr, lasso, rf, stack], ['SVM', 'Lasso','Random Forest','StackingClassifier']):\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Neg. MSE Score: %0.2f (+/- %0.2f) [%s]\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#标准Stacking 网格调参\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "X, y = load_boston(return_X_y=True)\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "rf = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "# The StackingCVRegressor uses scikit-learn's check_cv\n",
    "# internally, which doesn't support a random seed. Thus\n",
    "# NumPy's random seed need to be specified explicitely for\n",
    "# deterministic behavior\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "stack = StackingCVRegressor(regressors=(lasso, ridge),\n",
    "                            meta_regressor=rf,\n",
    "                            use_features_in_secondary=True)\n",
    "params = {'lasso__alpha': [0.1, 1.0, 10.0],\n",
    "          'ridge__alpha': [0.1, 1.0, 10.0]}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "                  estimator=stack,param_grid={'lasso__alpha': [x/5.0 for x in range(1, 10)],\n",
    "                'ridge__alpha': [x/20.0 for x in range(1, 10)],\n",
    "                'meta-randomforestregressor__n_estimators': [10,100]},\n",
    "    cv=5,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "#Best: 0.673590 using {'lasso__alpha': 0.4, 'meta-randomforestregressor__n_estimators': 10, 'ridge__alpha\n",
    "\n",
    "cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "    print(\"%0.3f +/- %0.2f %r\"\n",
    "          % (grid.cv_results_[cv_keys[0]][r],\n",
    "          grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "          grid.cv_results_[cv_keys[2]][r]))\n",
    "    if r > 10:\n",
    "    break\n",
    "print('...')\n",
    "\n",
    "print('Best parameters: %s' % grid.best_params_)\n",
    "print('Accuracy: %.2f' % grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "print(jieba.lcut(\"我在网易云课堂学习自然语言处理\"))\n",
    "print(jieba.lcut_for_search(\"小明硕士毕业于中国科学院计算所，后在斯坦福大学深造\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加自定义分词\n",
    "jieba.suggest_freq(('中', '将'), True)\n",
    "print('/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词性标注\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我在网易云课堂学习自然语言处理\")\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于TF-IDF的关键词抽取\n",
    "import jieba.analyse as analyse\n",
    "lines=open('data/NBA.txt','r',encoding='UTF-8').read()\n",
    "print(\"  \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于textrank的关键词抽取\n",
    "import jieba.analyse as analyse\n",
    "lines = open('data/NBA.txt','r',encoding='UTF-8').read()\n",
    "print(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按行读取文本数据\n",
    "# pandas读取数据\n",
    "df = pd.read_csv(\"./data/entertainment_news.csv\", encoding='utf-8').dropna()\n",
    "# 转成list\n",
    "content=df[\"content\"].values.tolist()\n",
    "# 分词与统计词频\n",
    "segment=[]\n",
    "for line in content:\n",
    "    try:\n",
    "        segs=jieba.lcut(line)\n",
    "        for seg in segs:\n",
    "            if len(seg)>1 and seg!='\\r\\n':\n",
    "                segment.append(seg)\n",
    "    except:\n",
    "        print(line)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去停用词\n",
    "words_df=pd.DataFrame({'segment':segment})\n",
    "stopwords=pd.read_csv(\"data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')#quoting=3全不引用\n",
    "words_df=words_df[~words_df.segment.isin(stopwords.stopword)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词频统计\n",
    "words_stat=words_df.groupby(by=['segment'])['segment'].agg({\"计数\":numpy.size})\n",
    "words_stat=words_stat.reset_index().sort_values(by=[\"计数\"],ascending=False)\n",
    "words_stat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词云\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "wordcloud=WordCloud(font_path=\"data/simhei.ttf\",background_color=\"black\",max_font_size=80)\n",
    "word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}\n",
    "wordcloud=wordcloud.fit_words(word_frequence)\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义背景\n",
    "from scipy.misc import imread\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "bimg=imread('image/entertainment.jpeg')\n",
    "wordcloud=WordCloud(background_color=\"white\",mask=bimg,font_path='data/simhei.ttf',max_font_size=200)\n",
    "word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}\n",
    "wordcloud=wordcloud.fit_words(word_frequence)\n",
    "bimgColors=ImageColorGenerator(bimg)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloud.recolor(color_func=bimgColors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在jupyternotebook中显示图片\n",
    "<img src=\"./image/LSTM.png\" width=\"500\" height=\"40\" align=center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

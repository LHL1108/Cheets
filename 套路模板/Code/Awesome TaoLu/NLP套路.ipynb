{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP套路模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参考文档《手把手教你在Python中实现文本分类（附代码、数据集）》  \n",
    "https://blog.csdn.net/Tw6cy6uKyDea86Z/article/details/80416475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据集预处理、特征工程和模型训练所需的库\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd, xgboost, numpy, textblob, string\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "\n",
    "file = open('data/corpus')\n",
    "data = file.read()\n",
    "\n",
    "labels, texts = [], []\n",
    "\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(content[1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建dataframe\n",
    "train_df = pd.DataFrame()\n",
    "train_df['text'] = texts\n",
    "train_df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_df['text'], train_df['label'], test_size=0.2)\n",
    "\n",
    "# 将标签变为0/1的label编码\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个向量计数器对象\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "\n",
    "count_vect.fit(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count = count_vect.transform(X_train)\n",
    "X_valid_count = count_vect.transform(X_valid)\n",
    "\n",
    "# 将每一个词转换成了一个统计词频的词袋模型\n",
    "print(X_train[0])\n",
    "print(X_train_count.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词语级Tf_idf\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(train_df['text'])\n",
    "X_train_tfidf = tfidf_model.transform(X_train)\n",
    "X_valid_tfidf = tfidf_model.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram级tf_idf\n",
    "ngram_tfidf_model = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=5000)\n",
    "ngram_tfidf_model.fit(train_df['text'])\n",
    "X_train_tfidf_ngram = ngram_tfidf_model.transform(X_train)\n",
    "X_valid_tfidf_ngram = ngram_tfidf_model.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符级tf_idf\n",
    "char_tfidf_model = TfidfVectorizer(analyzer='char', ngram_range=(1,2), max_features=5000)\n",
    "char_tfidf_model.fit(train_df['text'])\n",
    "X_train_tfidf_char = char_tfidf_model.transform(X_train)\n",
    "X_valid_tfidf_char = char_tfidf_model.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载预先训练好的词嵌入向量\n",
    "embeddings_index = {}\n",
    "\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "\n",
    "\n",
    "#创建一个分词器\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "\n",
    "\n",
    "#将文本转换为分词序列，并填充它们保证得到相同长度的向量\n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "\n",
    "\n",
    "#创建分词嵌入映射\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符计数特征\n",
    "train_df['char_count'] = train_df['text'].apply(len)\n",
    "\n",
    "# 单词计数特征\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x:len(x.split()))\n",
    "\n",
    "# 单词平均长度计数特征\n",
    "train_df['word_density'] = train_df['char_count'] / (train_df['word_count'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练主题模型\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter==20)\n",
    "X_topic = lda_model.fit_transform(X_train_count)\n",
    "topic_word = lda_model.components_\n",
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化主题模型\n",
    "n_top_words = 10 \n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(''.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个所有模型的标准模板\n",
    "def train_model(classifier, X_train, y_train, X_valid, y_valid, is_neural_net=False):\n",
    "    classifier.fit(feature_vector_train, y_train)\n",
    "    predictions = classifier.predict(X_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征为计数向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "#特征为词语级别TF-IDF向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), X_train_tfidf, y_train, X_valid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "#特征为多个词语级别TF-IDF向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_ngram, y_train, X_valid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "#特征为词性级别TF-IDF向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "#参考输出结果\n",
    "# NB, Count Vectors:  0.7004\n",
    "# NB, WordLevel TF-IDF:  0.7024\n",
    "# NB, N-Gram Vectors:  0.5344\n",
    "# NB, CharLevel Vectors:  0.6872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "#特征为词语级别TF-IDF向量的线性分类器\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "#特征为多个词语级别TF-IDF向量的线性分类器\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "#特征为词性级别TF-IDF向量的线性分类器\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "#参考输出结果\n",
    "# LR, Count Vectors:  0.7048\n",
    "# LR, WordLevel TF-IDF:  0.7056\n",
    "# LR, N-Gram Vectors:  0.4896\n",
    "# LR, CharLevel Vectors:  0.7012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征为多个词语级别TF-IDF向量的SVM\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "#输出结果\n",
    "#SVM, N-Gram Vectors:  0.5296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征为计数向量的RF\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "#特征为词语级别TF-IDF向量的RF\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "#输出结果\n",
    "# RF, Count Vectors:  0.6972\n",
    "# RF, WordLevel TF-IDF:  0.6988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征为计数向量的Xgboost\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print(\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "#特征为词语级别TF-IDF向量的Xgboost\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "#特征为词性级别TF-IDF向量的Xgboost\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print(\"Xgb, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "#输出结果\n",
    "# Xgb, Count Vectors:  0.6324\n",
    "# Xgb, WordLevel TF-IDF:  0.6364\n",
    "# Xgb, CharLevel Vectors:  0.6548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建浅层神经网络\n",
    "def create_model_architecture(input_size):\n",
    "    # create inputlayer\n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer \n",
    "    hidden_layer = layers.Dense(100, activation='relu')(input_layer)\n",
    "    \n",
    "    #create output layer\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
    "    \n",
    "    classifier = models.Model(inputs=input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print \"NN, Ngram Level TF IDF Vectors\",  accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建卷积神经网络\n",
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "    \n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accuracy)\n",
    "\n",
    "#输出结果\n",
    "# Epoch 1/1\n",
    "# 7500/7500 [==============================] - 12s 2ms/step - loss: 0.5847\n",
    "# CNN, Word Embeddings 0.5296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建LSTM\n",
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy)\n",
    "\n",
    "#输出结果\n",
    "# Epoch 1/1\n",
    "# 7500/7500 [==============================] - 22s 3ms/step - loss: 0.6899\n",
    "# RNN-LSTM, Word Embeddings 0.5124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建GRU\n",
    "def create_rnn_gru():\n",
    "   # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "  \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-GRU, Word Embeddings\",  accuracy)\n",
    "\n",
    "#输出结果\n",
    "# Epoch 1/1\n",
    "# 7500/7500 [==============================] - 19s 3ms/step - loss: 0.6898\n",
    "# RNN-GRU, Word Embeddings 0.5124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_rnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)\n",
    "\n",
    "#输出结果\n",
    "# Epoch 1/1\n",
    "# 7500/7500 [==============================] - 32s 4ms/step - loss: 0.6889\n",
    "# RNN-Bidirectional, Word Embeddings 0.5124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建RCNN\n",
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accuracy)\n",
    "\n",
    "#输出结果\n",
    "# Epoch 1/1\n",
    "# 7500/7500 [==============================] - 11s 1ms/step - loss: 0.6902\n",
    "# CNN, Word Embeddings 0.5124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 文本预处理套路\n",
    "- https://blog.csdn.net/wizardforcel/article/details/83933459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词袋模型\n",
    "# 加载库\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 创建文本\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# 创建词袋特征矩阵\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# 展示特征矩阵\n",
    "bag_of_words.toarray()\n",
    "\n",
    "'''\n",
    "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
    "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
    "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64) \n",
    "'''\n",
    "\n",
    "# 获取特征名称\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "# 查看特征名称\n",
    "feature_names\n",
    "\n",
    "# ['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden'] \n",
    "\n",
    "# 创建数据帧\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析HTML\n",
    "# 加载库\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 创建一些 HTML 代码\n",
    "html = \"<div class='full_name'><span style='font-weight:bold'>Masego</span> Azra</div>\"\n",
    "\n",
    "# 解析 html\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# 寻找带有 \"full_name\" 类的 <div>，展示文本\n",
    "soup.find(\"div\", { \"class\" : \"full_name\" }).text\n",
    "\n",
    "# 'Masego Azra' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除标点\n",
    "# 加载库\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# 创建文本\n",
    "text_data = ['Hi!!!! I. Love. This. Song....', \n",
    "             '10000% Agree!!!! #LoveIT', \n",
    "             'Right?!?!']\n",
    "\n",
    "# 创建函数，使用 string.punctuation 移除所有标点\n",
    "def remove_punctuation(sentence: str) -> str:\n",
    "    return sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# 应用函数\n",
    "[remove_punctuation(sentence) for sentence in text_data]\n",
    "\n",
    "# ['Hi I Love This Song', '10000 Agree LoveIT', 'Right'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除停用词\n",
    "# 加载库\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 你第一次需要下载停止词的集合\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "'''\n",
    "[nltk_data] Downloading package stopwords to\n",
    "[nltk_data]     /Users/chrisalbon/nltk_data...\n",
    "[nltk_data]   Package stopwords is already up-to-date!\n",
    "\n",
    "True \n",
    "'''\n",
    "\n",
    "# 创建单词标记\n",
    "tokenized_words = ['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']\n",
    "\n",
    "# 加载停止词\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# 展示停止词\n",
    "stop_words[:5]\n",
    "\n",
    "# ['i', 'me', 'my', 'myself', 'we'] \n",
    "\n",
    "# 移除停止词\n",
    "[word for word in tokenized_words if word not in stop_words]\n",
    "\n",
    "# ['going', 'go', 'store', 'park'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换字符\n",
    "# 导入库\n",
    "import re\n",
    "\n",
    "# 创建文本\n",
    "text_data = ['Interrobang. By Aishwarya Henriette',\n",
    "             'Parking And Going. By Karl Gautier',\n",
    "             'Today Is The night. By Jarek Prakash']\n",
    "\n",
    "# 移除句号\n",
    "remove_periods = [string.replace('.', '') for string in text_data]\n",
    "\n",
    "# 展示文本\n",
    "remove_periods\n",
    "\n",
    "'''\n",
    "['Interrobang By Aishwarya Henriette',\n",
    " 'Parking And Going By Karl Gautier',\n",
    " 'Today Is The night By Jarek Prakash'] \n",
    "'''\n",
    "\n",
    "# 创建函数\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r'[a-zA-Z]', 'X', string)\n",
    "\n",
    "# 应用函数\n",
    "[replace_letters_with_X(string) for string in remove_periods]\n",
    "\n",
    "'''\n",
    "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
    " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
    " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX'] \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词干提取\n",
    "# 加载库\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# 创建单词标记\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "\n",
    "# 创建提取器\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# 应用提取器\n",
    "[porter.stem(word) for word in tokenized_words]\n",
    "\n",
    "# ['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除空白\n",
    "# 创建文本\n",
    "text_data = ['   Interrobang. By Aishwarya Henriette     ',\n",
    "             'Parking And Going. By Karl Gautier',\n",
    "             '    Today Is The night. By Jarek Prakash   ']\n",
    "\n",
    "# 移除空白\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "# 展示文本\n",
    "strip_whitespace\n",
    "\n",
    "'''\n",
    "['Interrobang. By Aishwarya Henriette',\n",
    " 'Parking And Going. By Karl Gautier',\n",
    " 'Today Is The night. By Jarek Prakash'] \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词性标签\n",
    "# 加载库\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# 创建文本\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# 使用预训练的词性标注器\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# 展示词性\n",
    "text_tagged\n",
    "\n",
    "# [('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "# 加载库\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 创建文本\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# 创建 tf-idf 特征矩阵\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# 展示 tf-idf 特征矩阵\n",
    "feature_matrix.toarray()\n",
    "\n",
    "'''\n",
    "array([[ 0.        ,  0.        ,  0.        ,  0.89442719,  0.        ,\n",
    "         0.        ,  0.4472136 ,  0.        ],\n",
    "       [ 0.        ,  0.57735027,  0.        ,  0.        ,  0.        ,\n",
    "         0.57735027,  0.        ,  0.57735027],\n",
    "       [ 0.57735027,  0.        ,  0.57735027,  0.        ,  0.57735027,\n",
    "         0.        ,  0.        ,  0.        ]]) \n",
    "'''\n",
    "\n",
    "# 展示 tf-idf 特征矩阵\n",
    "tfidf.get_feature_names()\n",
    "\n",
    "# ['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden'] \n",
    "\n",
    "# 创建数据帧\n",
    "pd.DataFrame(feature_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本分词\n",
    "# 加载库\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# 创建文本\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "# 对文本分词\n",
    "word_tokenize(string)\n",
    "\n",
    "'''\n",
    "['The',\n",
    " 'science',\n",
    " 'of',\n",
    " 'today',\n",
    " 'is',\n",
    " 'the',\n",
    " 'technology',\n",
    " 'of',\n",
    " 'tomorrow',\n",
    " '.',\n",
    " 'Tomorrow',\n",
    " 'is',\n",
    " 'today',\n",
    " '.'] \n",
    "'''\n",
    "\n",
    "# 对句子分词\n",
    "sent_tokenize(string)\n",
    "\n",
    "# ['The science of today is the technology of tomorrow.', 'Tomorrow is today.'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'john': 4, 'likes': 5, 'ice': 3, 'cream': 1, 'hates': 2, 'chocolate': 0}\n",
      "[[0 1 0 1 2 1]\n",
      " [1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 词袋模型 CountVectorizer  (一个小例子)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = ['John likes ice cream John', 'John hates chocolate.']\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=0, lowercase=True)\n",
    "vectorizer.fit(sentences)\n",
    "# CountVectorizer训练将出现的每一个词建立索引并计算频次,注意，频次是指在1篇文档下的频次！\n",
    "print(vectorizer.vocabulary_)\n",
    "# CountVectorizer将出现的句子根据对应的索引和词频进行向量化表示\n",
    "print(vectorizer.transform(sentences).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制学习曲线\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用CountVectorizer，每个向量的长度相同，等于总语料库的大小，其中的值表示该词在这句话中的出现次数。\n",
    "# 使用Tokenizer，每个向量等于每个文本的长度，其数值并不表示计数，而是对应于字典tokenizer.word_index中的单词值。\n",
    "# 词嵌入\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_valid = tokenizer.texts_to_sequences(sentences_valid)\n",
    "\n",
    "# vocabsize一定程度代表了tokenizer的一个值最大有可能是多少\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# tokenizer把每一个词转化成了一个字典的key，这样就把一句话进行了稠密表示，然而这种方式并不能表征词与词之间的关系\n",
    "print(sentences_train[0])\n",
    "print(X_train[0])\n",
    "\n",
    "# 可以看到'out'对应的Value为35\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对长度不一的Tokenizer进行补齐操作\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# maxlen一定程度代表了这句话最长可能由多少个单词组成\n",
    "maxlen = 100 \n",
    "print(sentences_train[:2])\n",
    "#首先看一下X_train,此时由每句话转成的向量长度不等\n",
    "print(X_train[:2])\n",
    "\n",
    "# 使用pad_sequence()进行补齐,maxlen控制补齐后的长度\n",
    "print('开始填充序列...')\n",
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_valid = pad_sequences(X_valid, maxlen=100)\n",
    "print(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras实现Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# 让每一个词由50维的稠密向量表示，这样我们就由不可学习的tokenizer稠密表示\n",
    "# 变为可学习的embedding稠密表示\n",
    "embedding_dim = 50 \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 构建嵌入层，注意各参数对应的指标\n",
    "model.add(Embedding(input_dim=vocab_size,     # 词典大小，每一个值的最大值\n",
    "                    output_dim=embedding_dim, # 每一个词用多大维度的稠密向量来表示\n",
    "                    input_length=maxlen))     # 输入层的维度\n",
    "# 将嵌入层平铺的展开层\n",
    "model.add(Flatten())\n",
    "# 是个神经元的隐藏层\n",
    "model.add(Dense(10, activation='relu'))\n",
    "# 激活函数为sigmoid的输出层\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 编译模型，设定优化器，损失函数，评估指标\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# 查看一下该模型\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    verbose=1,                    \n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_valid, y_valid, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加池化层\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GlobalMaxPool1D, Dense\n",
    "\n",
    "embedding_dim = 50 \n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=maxlen))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习使用预训练好的Glove词嵌入\n",
    "# 首先下载词嵌入,并针对word_index中出现的词做筛选\n",
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "# 创建嵌入矩阵\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('./data/glove.6B.50d.txt',tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# 统计Glove对本预料的覆盖比\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print('{}%的向量是非零的,被预训练的词嵌入模型覆盖'.format(round((nonzero_elements / vocab_size) * 100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用预训练词嵌入的DNN模型训练\n",
    "model = Sequential() \n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix], # 将词嵌入矩阵赋值给嵌入层\n",
    "                    trainable=False))            # 参数不可更新    \n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    verbose=1,                    \n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_valid, y_valid, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再试下fine tuning更新词嵌入模型\n",
    "model = Sequential() \n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=maxlen,\n",
    "                    weights=[embedding_matrix], # 将词嵌入矩阵赋值给嵌入层\n",
    "                    trainable=True))            # 参数不可更新    \n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCNN\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D\n",
    "model = Sequential() \n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=maxlen))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机搜索调节参数，为了让keras可以应用sklearn中的随机搜索\n",
    "# 需要将keras包装成KerasClassifier\n",
    "def Create_model(num_filters, kernel_size, vocab_size, maxlen):\n",
    "    model = Sequential() \n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "    model.add(Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(num_filters=[64, 128],\n",
    "                  kernel_size=[5, 7],\n",
    "                  vocab_size=[5000],\n",
    "                  embedding_dim=[50],\n",
    "                  maxlen=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机搜索对Text CNN 进行调参\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Main settings \n",
    "epochs = 20 \n",
    "embedding_dim = 50\n",
    "maxlen = 100 \n",
    "output_file = 'data/output.txt'\n",
    "\n",
    "# Run grid search for each source (yelp, amazon, imdb)\n",
    "for source, fram in df.groupby('source'):\n",
    "    print('Running grid search for dataset:', source)\n",
    "    sentences = df['sentence'].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    # Train test split\n",
    "    sentences_train, sentences_valid, y_train, y_valid = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
    "    \n",
    "    # Tokenize words \n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    X_valid = tokenizer.texts_to_sequences(sentences_train)\n",
    "    \n",
    "    # Adding 1 because of 0 in index\n",
    "    vocab_size = len(tokenizer.word_index) + 1 \n",
    "    \n",
    "    # Pad sequences with zeros \n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)  # padding 在起始补还是在结尾补,truncating当需要截断序列时，从起始还是结尾截断\n",
    "    X_valid = pad_sequences(X_valid, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    # Parameter grid for grid search \n",
    "    param_grid = dict(num_filters=[32, 64, 128],\n",
    "                      kernel_size=[1, 3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      maxlen=[maxlen])\n",
    "    model = KerasClassifier(build_fn=Create_model,\n",
    "                            epochs=10, \n",
    "                            batch_size=10,\n",
    "                            verbose=True)\n",
    "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,cv=2, verbose=1)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate testing set \n",
    "    valid_accuracy = grid.score(X_valid, y_valid)\n",
    "    \n",
    "        # Save and evaluate results\n",
    "    prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
    "    if prompt.lower() not in {'y', 'true', 'yes'}:\n",
    "        break\n",
    "    with open(output_file, 'a') as f:\n",
    "        s = ('Running {} data set\\nBest Accuracy : '\n",
    "             '{:.4f}\\n{}\\nValid Accuracy : {:.4f}\\n\\n')\n",
    "        output_string = s.format(\n",
    "            source,\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_,\n",
    "            valid_accuracy)\n",
    "        print(output_string)\n",
    "        f.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec\n",
    "实例：\n",
    "#导入word2vec模块\n",
    "from gensim.models import Word2Vec\n",
    "#训练word2vec模型\n",
    "model = Word2Vec(LineSentence('word2vec.txt'),min_count=1,size=200,iter=10)\n",
    "#获取每个词和其表征向量\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "# 通常来说在该方法在大量语料的时候能获得更好的效果，word2vec.txt是每条记录的分词结果，分词结果用空格隔开，每条记录用回车区分，如：\n",
    "# 踢 下线 国际社区支局\n",
    "# 合同号 推送 电子发票 可否 天宫殿支局\n",
    "# 回不了笼 普子支局\n",
    "在训练之后，可以先把模型保存下来再导入，如：\n",
    "model.save('word2vec')\n",
    "model = Word2Vec.load('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在jupyter notebook中显示图片\n",
    "<img src=\"./image/LSTM.png\" width=\"500\" height=\"40\" align=center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
